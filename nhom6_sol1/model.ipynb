{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lay data tu file excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/content/drive/MyDrive/data/tiki_final.xlsx')\n",
    "df.rename(columns={'product': 'productName'}, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst = list(df['label'].unique())\n",
    "\n",
    "sampled_df = pd.DataFrame().reindex_like(df)[0:0]\n",
    "\n",
    "for label in label_lst:\n",
    "  df_label = df[df['label'] == label].sample(40)\n",
    "  sampled_df = pd.concat([sampled_df, df_label])\n",
    "df = sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = df.label.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict\n",
    "df['Label'] = df.label.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "                                                  df.label.values,\n",
    "                                                  test_size=0.15,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'Label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].productName.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].productName.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].Label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].Label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val,\n",
    "                                   sampler=SequentialSampler(dataset_val),\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "epochs = 7\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "if not os.path.exists('/content/drive/MyDrive/Colab Notebooks/Bert_classification/fineTuned/'):\n",
    "    os.makedirs('/content/drive/MyDrive/Colab Notebooks/Bert_classification/fineTuned/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/Bert_classification/fineTuned/checkpoint_epoch_29.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "for epoch in tqdm(range(30, epochs+30)):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict':optimizer.state_dict(),\n",
    "        'epoch':epoch\n",
    "    }, f'/content/drive/MyDrive/Colab Notebooks/Bert_classification/fineTuned/checkpoint_epoch_{epoch}.pth')\n",
    "\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('label2id.json', 'w') as json_file:\n",
    "    json.dump(label_dict, json_file)\n",
    "    \n",
    "with open('label2id.json', 'r') as json_file:\n",
    "    label2id = json.load(json_file)\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label2id),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/Bert_classification/fineTuned/checkpoint_epoch_31.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def predict(text):\n",
    "  input = tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      return_attention_mask=True,\n",
    "      pad_to_max_length=True,\n",
    "      max_length=256,\n",
    "      truncation=True,\n",
    "      return_tensors='pt'\n",
    "  )\n",
    "  input.to(device)\n",
    "  return id2label[int(torch.argmax(model(**input).logits.cpu()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/content/drive/MyDrive/data/product_test_unlabeled.xlsx')\n",
    "df.rename(columns={'product': 'productName'}, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.head()\n",
    "model.to(device)\n",
    "predictlabels = []\n",
    "for text in df['productName']:\n",
    "    predictlabels.append(predict(text))\n",
    "df['predictLabel'] = predictlabels\n",
    "\n",
    "df.to_excel('/content/drive/MyDrive/data/product_test_result_3.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/content/drive/MyDrive/data/tiki_final.xlsx\"\n",
    "# data = pd.read_excel(file_path, header=None)\n",
    "# data.dropna(inplace= True)\n",
    "# data.columns = [\"labels\", \"text\"]\n",
    "\n",
    "# # data['labels'] = data['labels'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# X = data[\"text\"]\n",
    "# Y = data[\"labels\"]\n",
    "# yt=[]\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# print(type(Y))\n",
    "# yt=mlb.fit_transform([Y.to_list()])\n",
    "\n",
    "# print(yt[14])\n",
    "\n",
    "# print(mlb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_temp, y_train_temp, X_test, y_test_ = iterative_train_test_split(np.vstack(X.to_numpy()), yt, test_size = 0.3)\n",
    "\n",
    "# X_train_, y_train_, X_val, y_val_ = iterative_train_test_split(X_train_temp, y_train_temp, test_size = 0.2)\n",
    "\n",
    "# X_test = pd.Series(X_test.flatten())\n",
    "# y_test = pd.Series( (v for v in y_test_.tolist()) )\n",
    "# X_val = pd.Series(X_val.flatten())\n",
    "# y_val = pd.Series( (v for v in y_val_.tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path2 = \"libroprimo-capi.csv\"\n",
    "# data2 = pd.read_csv(file_path2, sep=\"\\t\", header=None)\n",
    "# data2.columns = [\"text\", \"capi\"]\n",
    "# data2['capi'] = data2['capi'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# articles = data2[\"text\"].to_numpy()\n",
    "# labels = data2[\"capi\"]\n",
    "\n",
    "# yt=[]\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "# yt=mlb.fit_transform(labels.to_list())\n",
    "\n",
    "# articles = articles.reshape(252, 1)\n",
    "\n",
    "# X_train = np.concatenate((X_train_, articles))\n",
    "# y_train = np.concatenate((y_train_, yt))\n",
    "\n",
    "# X_train = pd.Series(X_train.flatten())\n",
    "# y_train = pd.Series( (v for v in y_train.tolist()) )\n",
    "\n",
    "# print(\"----- Train -----\")\n",
    "\n",
    "# print(len(X_train))\n",
    "\n",
    "# print(\"----- Validation -----\")\n",
    "\n",
    "# print(len(X_val))\n",
    "\n",
    "# print(\"----- Test -----\")\n",
    "\n",
    "# print(len(X_test))\n",
    "\n",
    "# N_CLASSES=len(mlb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLabelDataset(Dataset):\n",
    "#     def __init__(self, article, tags, tokenizer, max_length):\n",
    "#       self.tokenizer=tokenizer\n",
    "#       self.text=article\n",
    "#       self.labels=tags\n",
    "#       self.max_length=max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#       return len(self.text)\n",
    "\n",
    "#     def __getitem__(self, item_id):\n",
    "#       text=self.text[item_id]\n",
    "#       inputs=self.tokenizer.encode_plus(\n",
    "#           text,\n",
    "#           add_special_tokens = True,\n",
    "#           max_length = self.max_length,\n",
    "#           pad_to_max_length = True,\n",
    "#           return_token_type_ids=True,\n",
    "#           return_attention_mask = True,\n",
    "#           truncation=True,                # tronca gli input con lunghezza minore di quella massima\n",
    "#           return_tensors = 'pt'\n",
    "#       )\n",
    "#       input_ids = inputs['input_ids'].flatten()\n",
    "#       attn_mask = inputs['attention_mask'].flatten()\n",
    "\n",
    "#       return {\n",
    "#           'input_ids': input_ids ,\n",
    "#           'attention_mask': attn_mask,\n",
    "#           'label': torch.tensor(self.labels[item_id], dtype=torch.float)\n",
    "#       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLabelDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, x_train, y_train, x_val, y_val, x_test, y_test, tokenizer, batch_size=16, max_token=512):\n",
    "#         super().__init__()\n",
    "#         self.training_text=x_train\n",
    "#         self.training_labels=y_train\n",
    "#         self.valuation_text=x_val\n",
    "#         self.valuation_labels=y_val\n",
    "#         self.test_text=x_test\n",
    "#         self.test_labels=y_test\n",
    "#         self.tokenizer=tokenizer\n",
    "#         self.batch_size=batch_size\n",
    "#         self.max_token=max_token\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         self.training_dataset=MultiLabelDataset(article=self.training_text, tags=self.training_labels, tokenizer=self.tokenizer, max_length=self.max_token)\n",
    "#         self.validation_dataset=MultiLabelDataset(article=self.valuation_text, tags=self.valuation_labels, tokenizer=self.tokenizer, max_length=self.max_token)\n",
    "#         self.test_dataset=MultiLabelDataset(article=self.test_text, tags=self.test_labels, tokenizer=self.tokenizer, max_length=self.max_token)\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.training_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.validation_dataset, batch_size=self.batch_size)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLabelClassifier(pl.LightningModule):\n",
    "#     def __init__(self, n_classes=N_CLASSES, n_epochs=NUM_EPOCHS, steps_per_epoch=None, learning_rate=3e-5):\n",
    "#         super().__init__()\n",
    "#         self.bert=BertModel.from_pretrained(BERT_MODEL, return_dict=True) # recupero modello preaddestrato\n",
    "#         self.classifier=nn.Linear(self.bert.config.hidden_size, n_classes) # applico un classificatore lineare\n",
    "#         self.steps_per_epoch=steps_per_epoch\n",
    "#         self.n_epochs=n_epochs\n",
    "#         self.learning_rate=learning_rate\n",
    "#         self.criterion=nn.BCELoss()\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, labels=None):\n",
    "#         output=self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         output=self.classifier(output.pooler_output) # questo ritorna il classification token dopo averlo processato attraverso un linear layer con funzione di attivazione tanh\n",
    "#         output=torch.sigmoid(output)\n",
    "#         loss=0\n",
    "#         if labels is not None:\n",
    "#             loss=self.criterion(output, labels)\n",
    "\n",
    "#         return loss, output\n",
    "\n",
    "#     def training_step(self,batch,batch_idx):\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['label']\n",
    "\n",
    "#         #outputs = self(input_ids,attention_mask)\n",
    "#         #loss = self.criterion(outputs, labels)\n",
    "\n",
    "#         loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "#         self.log('train_loss',loss , prog_bar=True,logger=True)\n",
    "\n",
    "#         return {\"loss\" :loss, \"predictions\":outputs, \"labels\": labels }\n",
    "\n",
    "\n",
    "#     def validation_step(self,batch,batch_idx):\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['label']\n",
    "\n",
    "#         #outputs = self(input_ids,attention_mask)\n",
    "#         #loss = self.criterion(outputs,labels)\n",
    "\n",
    "#         loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "#         self.log('val_loss',loss , prog_bar=True,logger=True)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def test_step(self,batch,batch_idx):\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['label']\n",
    "\n",
    "#         #outputs = self(input_ids,attention_mask)\n",
    "#         #loss = self.criterion(outputs,labels)\n",
    "\n",
    "#         loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "#         self.log('test_loss',loss , prog_bar=True,logger=True)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = AdamW(self.parameters() , lr=self.learning_rate)\n",
    "#         warmup_steps = self.steps_per_epoch//3\n",
    "#         total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "\n",
    "#         scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\n",
    "\n",
    "#         return dict(\n",
    "#             optimizer=optimizer,\n",
    "#             lr_scheduler=dict(\n",
    "#                 scheduler=scheduler,\n",
    "#                 interval='step'\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# BATCH_SIZE=8\n",
    "# learning_rate=3e-5\n",
    "\n",
    "# # Pretrained Tokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "\n",
    "# data_module = MultiLabelDataModule(X_train, y_train, X_val, y_val, X_test, y_test, tokenizer, BATCH_SIZE, 512)\n",
    "# data_module.setup()\n",
    "\n",
    "# step_per_epoch=len(X_train)//BATCH_SIZE\n",
    "\n",
    "# # Model definition\n",
    "\n",
    "# model=MultiLabelClassifier(n_classes=N_CLASSES, n_epochs=NUM_EPOCHS, steps_per_epoch=step_per_epoch, learning_rate=learning_rate)\n",
    "\n",
    "# # checkpoint\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor='val_loss',# monitored quantity\n",
    "#     filename='Task-{epoch:02d}-{val_loss:.2f}',\n",
    "#     save_top_k=3, #  save the top 3 models\n",
    "#     mode='min', # mode of the monitored quantity  for optimization\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(max_epochs=NUM_EPOCHS, accelerator=\"gpu\", log_every_n_steps=45, callbacks=[checkpoint_callback])\n",
    "# trainer.fit(model, data_module)\n",
    "\n",
    "# trainer.test(model,datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "# # Tokenize all questions in x_test\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "\n",
    "# for quest in X_test:\n",
    "#     encoded_quest =  tokenizer.encode_plus(\n",
    "#                     quest,\n",
    "#                     None,\n",
    "#                     add_special_tokens=True,\n",
    "#                     max_length=512,\n",
    "#                     padding = 'max_length',\n",
    "#                     return_token_type_ids= False,\n",
    "#                     return_attention_mask= True,\n",
    "#                     truncation=True,\n",
    "#                     return_tensors = 'pt'\n",
    "#     )\n",
    "\n",
    "#     # Add the input_ids from encoded question to the list.\n",
    "#     input_ids.append(encoded_quest['input_ids'])\n",
    "#     # Add its attention mask\n",
    "#     attention_masks.append(encoded_quest['attention_mask'])\n",
    "\n",
    "# # Now convert the lists into tensors.\n",
    "# input_ids = torch.cat(input_ids, dim=0)\n",
    "# attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(y_test)\n",
    "\n",
    "# # Set the batch size.\n",
    "# TEST_BATCH_SIZE = 64\n",
    "\n",
    "# # Create the DataLoader.\n",
    "# pred_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "# pred_sampler = SequentialSampler(pred_data)\n",
    "# pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "# flat_pred_outs = 0\n",
    "# flat_true_labels = 0\n",
    "\n",
    "# # Put model in evaluation mode\n",
    "# model = model.to(device) # moving model to cuda\n",
    "# model.eval()\n",
    "\n",
    "# # Tracking variables\n",
    "# pred_outs, true_labels = [], []\n",
    "\n",
    "# # Predict\n",
    "# for batch in pred_dataloader:\n",
    "#     # Add batch to GPU\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "#     # Unpack the inputs from our dataloader\n",
    "#     b_input_ids, b_attn_mask, b_labels = batch\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass, calculate logit predictions\n",
    "#         _, pred_out = model(b_input_ids,b_attn_mask)\n",
    "#         #pred_out = torch.sigmoid(pred_out)\n",
    "#         # Move predicted output and labels to CPU\n",
    "#         pred_out = pred_out.detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "#     pred_outs.append(pred_out)\n",
    "#     true_labels.append(label_ids)\n",
    "\n",
    "# # combining all values in a single list\n",
    "\n",
    "# pred_outs = np.concatenate(pred_outs, axis=0)\n",
    "\n",
    "# true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# print(pred_outs[0])\n",
    "# print(true_labels[0])\n",
    "\n",
    "# y_true=true_labels.ravel()\n",
    "\n",
    "# #  converting the probabilities according to the threshold\n",
    "\n",
    "# y_temp=[]\n",
    "# for predicted_row in pred_outs:\n",
    "#   temp=[]\n",
    "#   for tag_label in predicted_row:\n",
    "#     if tag_label>=THRESHOLD:\n",
    "#       temp.append(1)\n",
    "#     else:\n",
    "#       temp.append(0)\n",
    "#   y_temp.append(temp)\n",
    "# y_pred=np.array(y_temp).ravel() # converting in a monodimensional array\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_true)\n",
    "\n",
    "# #######################################################\n",
    "# #   METRICS\n",
    "# #######################################################\n",
    "\n",
    "# macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n",
    "# micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
    "# weighted_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n",
    "# macro_precision = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "# micro_precision = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "# weighted_precision = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "# macro_recall = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "# micro_recall = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "# weighted_recall = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "\n",
    "# print(\"Macro-F1 score: \" + str(macro_f1*100))\n",
    "# print(\"micro-F1 score: \" + str(micro_f1*100))\n",
    "# print(\"Weighted-F1 score: \" + str(weighted_f1*100))\n",
    "# print(\"Macro-Precision score: \" + str(macro_precision*100))\n",
    "# print(\"micro-Precision score: \" + str(micro_precision*100))\n",
    "# print(\"Weighted-Precision score: \" + str(weighted_precision*100))\n",
    "# print(\"Macro-Recall score: \" + str(macro_recall*100))\n",
    "# print(\"micro-Recall score: \" + str(micro_recall*100))\n",
    "# print(\"Weighted-Recall score: \" + str(weighted_recall*100))\n",
    "\n",
    "# #######################################################\n",
    "# #   INFERENCE\n",
    "# #######################################################\n",
    "\n",
    "# inference_text=\"ai fini dell'applicabilità della dirimente del vizio parziale di mente per i fatti commessi in stato di cronica intossicazione da sostanze stupefacenti non costituisce elemento di prova relativo ad un preesistente stato patologico del soggetto, il fatto che l'imputato sia stato, in precedenza, dichiarato non punibile, per l'ipotesi dell'acquisto o detenzione di modiche quantità di sostanze stupefacenti o psicotrope allo scopo di farne uso personale non terapeutico; pertanto non vi è obbligo per il giudice di disporre perizia.\"\n",
    "# inference_text2=\"in tema di patteggiamento, la declaratoria di estinzione del reato conseguente al decorso dei termini e al verificarsi delle condizioni previste dall'articolo 445 codice procedura penale comporta l'estinzione degli effetti penali anche ai fini della recidiva.\"\n",
    "\n",
    "# model_path = checkpoint_callback.best_model_path\n",
    "# model = MultiLabelClassifier.load_from_checkpoint(model_path)\n",
    "\n",
    "# #Function to Predict Tags from a Question\n",
    "# def predict(question):\n",
    "#     text_enc = tokenizer.encode_plus(\n",
    "#                     question,\n",
    "#                     None,\n",
    "#                     add_special_tokens=True,\n",
    "#                     max_length=512,\n",
    "#                     padding = 'max_length',\n",
    "#                     return_token_type_ids= False,\n",
    "#                     return_attention_mask= True,\n",
    "#                     truncation=True,\n",
    "#                     return_tensors = 'pt'\n",
    "#     )\n",
    "#     _, outputs = model(text_enc['input_ids'], text_enc['attention_mask'])\n",
    "#     pred_out = outputs[0].detach().numpy()\n",
    "#     print(pred_out)\n",
    "#     preds = [(pred > THRESHOLD) for pred in pred_out ]\n",
    "#     preds = np.asarray(preds)\n",
    "#     new_preds = preds.reshape(1,-1).astype(int)\n",
    "#     pred_tags = mlb.inverse_transform(new_preds)\n",
    "#     return pred_tags\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(inference_text)\n",
    "\n",
    "# tags = predict(inference_text)\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(tags)\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(X_test[0])\n",
    "\n",
    "# tags = predict(X_test[0])\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(tags)\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(inference_text2)\n",
    "\n",
    "# tags = predict(inference_text2)\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(tags)\n",
    "\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# df = sampled_df\n",
    "# X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "#                                                   df.label_index.values,\n",
    "#                                                   test_size=0.15,\n",
    "#                                                   random_state=42,\n",
    "#                                                   stratify=df.label_index.values)\n",
    "\n",
    "# df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "# df.loc[X_train, 'data_type'] = 'train'\n",
    "# df.loc[X_val, 'data_type'] = 'val'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
